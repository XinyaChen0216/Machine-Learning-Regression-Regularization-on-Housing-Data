{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 52772\n"
     ]
    }
   ],
   "source": [
    "with open(\"seed.txt\", \"r\") as f:\n",
    "    seed = int(f.read().strip())\n",
    "\n",
    "print(f\"seed = {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 506\n",
      "Train set: 303 rows, Validation set: 101 rows, Test set: 102\n",
      "Train: 59.9%,  Validation: 20.0%,  Test: 20.2%\n"
     ]
    }
   ],
   "source": [
    "#Task 1: Regression — Regularization on Housing Data\n",
    "#1.1 1A. Data Preprocessing (Required)\n",
    "#1. Use your seed to create a 60/20/20 train/validation/test split.\n",
    "\n",
    "#load dataset\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"https://lib.stat.cmu.edu/datasets/boston\"\n",
    "response = urllib.request.urlopen(url)\n",
    "raw = response.read().decode()\n",
    "lines = raw.strip().splitlines()\n",
    "data_lines = lines[22:] #dataset starts from line 23 \n",
    "\n",
    "dataset = []\n",
    "\n",
    "#each data contains 2 lines\n",
    "for i in range(0, len(data_lines), 2):\n",
    "    line1 = data_lines[i].strip().split()\n",
    "    line2 = data_lines[i+1].strip().split()\n",
    "    dataset.append(line1 + line2)\n",
    "\n",
    "column_names = [\n",
    "    \"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\",\n",
    "    \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(dataset, columns=column_names).astype(float)\n",
    "\n",
    "#display(df)\n",
    "\n",
    "total_rows = len(df)\n",
    "print(f\"Total rows in dataset: {total_rows}\")\n",
    "\n",
    "np.random.seed(seed)\n",
    "indices = np.random.permutation(total_rows)\n",
    "\n",
    "train_end = int(0.6 * total_rows)\n",
    "validation_end = int(0.8 * total_rows)\n",
    "\n",
    "train_index = indices[:train_end] #60%\n",
    "validation_index   = indices[train_end:validation_end] #20%\n",
    "test_index  = indices[validation_end:] #20%\n",
    "\n",
    "train_df = df.iloc[train_index]\n",
    "validation_df   = df.iloc[validation_index]\n",
    "test_df  = df.iloc[test_index]\n",
    "\n",
    "#verify split correctly\n",
    "print(f\"Train set: {len(train_df)} rows,\", f\"Validation set: {len(validation_df)} rows,\", f\"Test set: {len(test_df)}\")\n",
    "print(f\"Train: {len(train_df)/len(df)*100:.1f}%, \", f\"Validation: {len(validation_df)/len(df)*100:.1f}%, \", f\"Test: {len(test_df)/len(df)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#2. Handle missing values (if any) and apply appropriate transformations:\n",
    "#• If categorical columns exist: apply one-hot encoding.\n",
    "\n",
    "df.isnull().sum()\n",
    "#So there is no missing values in this dataset\n",
    "\n",
    "categorical_columns = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "print(categorical_columns)\n",
    "#categorical_columns is empty, so there is no categorical columns in this dataset\n",
    "\n",
    "\n",
    "#• Create two interaction features of your choice; justify why they may help.\n",
    "\n",
    "# RM(average number of rooms per dwelling)and LSTAT(% lower status of the population) are two interaction features. \n",
    "# In a high-status neighborhood, where LSTAT is low, an extra room can add a lot of value. However, in a low-status area,\n",
    "# where LSTAT is high, an extra room may only add little value. So in the model, we want to show that the effect of room \n",
    "# numbers depends on the LSTAT. Adding the interaction RM × LSTAT to capture non-additive effects between house size and \n",
    "# neighborhood socioeconomic status can improve model expressiveness. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Scale:  (303, 14) (101, 14) (102, 14)\n",
      "Standard Scale:  (303, 14) (101, 14) (102, 14)\n",
      "No scale:  (303, 14) (101, 14) (102, 14)\n"
     ]
    }
   ],
   "source": [
    "#3. Run the full pipeline under three scaling settings:\n",
    "\n",
    "X_train = train_df.to_numpy(dtype=float)\n",
    "X_validation = validation_df.to_numpy(dtype=float)\n",
    "X_test = test_df.to_numpy(dtype=float)\n",
    "\n",
    "#(a) No scaling\n",
    "train_no_scale, validation_no_scale, test_no_scale = X_train, X_validation, X_test\n",
    "\n",
    "#(b) Standard scaling (z-score)\n",
    "def standard_scaler_fit(X_train):\n",
    "    mu = X_train.mean(axis=0)\n",
    "    sigma = X_train.std(axis=0, ddof=0)\n",
    "    sigma[sigma == 0] = 1.0  # signma cannot be zero\n",
    "    return mu, sigma\n",
    "\n",
    "def standard_scaler_transform(X, mu, sigma):\n",
    "    return (X - mu) / sigma\n",
    "\n",
    "mu, sigma = standard_scaler_fit(X_train)\n",
    "\n",
    "train_standard_scaling = standard_scaler_transform(X_train, mu, sigma)\n",
    "validation_standard_scaling = standard_scaler_transform(X_validation, mu, sigma)\n",
    "test_standard_scaling = standard_scaler_transform(X_test, mu, sigma)\n",
    "\n",
    "#(c) Robust scaling (median/IQR)\n",
    "def robust_scaler_fit(X_train):\n",
    "    median = np.median(X_train, axis=0)\n",
    "    q1 = np.percentile(X_train, 25, axis=0)\n",
    "    q3 = np.percentile(X_train, 75, axis =0)\n",
    "    iqr =  q3 - q1\n",
    "    iqr[iqr == 0] = 1 # iqr cannot be zero\n",
    "    return median, iqr\n",
    "\n",
    "def robust_scaler_transform(X, median, iqr):\n",
    "    return (X-median) / iqr\n",
    "\n",
    "median, iqr = standard_scaler_fit(X_train)\n",
    "\n",
    "train_robust_scaling = robust_scaler_transform(X_train, median, iqr)\n",
    "validation_robust_scaling = robust_scaler_transform(X_validation, median, iqr)\n",
    "test_robust_scaling = robust_scaler_transform(X_test, median, iqr)\n",
    "\n",
    "print(\"No Scaling: \", train_no_scale.shape, validation_no_scale.shape, test_no_scale.shape)\n",
    "print(\"Standard Scaling: \", train_no_scale.shape, validation_no_scale.shape, test_no_scale.shape)\n",
    "print(\"Robust Scaling: \", train_no_scale.shape, validation_no_scale.shape, test_no_scale.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
