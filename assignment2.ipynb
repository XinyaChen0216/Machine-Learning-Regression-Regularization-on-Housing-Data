{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seed = 52772\n"
          ]
        }
      ],
      "source": [
        "with open(\"seed.txt\", \"r\") as f:\n",
        "    seed = int(f.read().strip())\n",
        "\n",
        "print(f\"seed = {seed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rows in dataset: 506\n",
            "Train set: 303 rows, Validation set: 101 rows, Test set: 102\n",
            "Train: 59.9%,  Validation: 20.0%,  Test: 20.2%\n"
          ]
        }
      ],
      "source": [
        "#Task 1: Regression — Regularization on Housing Data\n",
        "#1.1 1A. Data Preprocessing (Required)\n",
        "#1. Use your seed to create a 60/20/20 train/validation/test split.\n",
        "\n",
        "#load dataset\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://lib.stat.cmu.edu/datasets/boston\"\n",
        "response = urllib.request.urlopen(url)\n",
        "raw = response.read().decode()\n",
        "lines = raw.strip().splitlines()\n",
        "data_lines = lines[22:] #dataset starts from line 23 \n",
        "\n",
        "dataset = []\n",
        "\n",
        "#each data contains 2 lines\n",
        "for i in range(0, len(data_lines), 2):\n",
        "    line1 = data_lines[i].strip().split()\n",
        "    line2 = data_lines[i+1].strip().split()\n",
        "    dataset.append(line1 + line2)\n",
        "\n",
        "column_names = [\n",
        "    \"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\",\n",
        "    \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(dataset, columns=column_names).astype(float)\n",
        "\n",
        "#display(df)\n",
        "\n",
        "total_rows = len(df)\n",
        "print(f\"Total rows in dataset: {total_rows}\")\n",
        "\n",
        "np.random.seed(seed)\n",
        "indices = np.random.permutation(total_rows)\n",
        "\n",
        "train_end = int(0.6 * total_rows)\n",
        "validation_end = int(0.8 * total_rows)\n",
        "\n",
        "train_index = indices[:train_end] #60%\n",
        "validation_index   = indices[train_end:validation_end] #20%\n",
        "test_index  = indices[validation_end:] #20%\n",
        "\n",
        "train_df = df.iloc[train_index]\n",
        "validation_df   = df.iloc[validation_index]\n",
        "test_df  = df.iloc[test_index]\n",
        "\n",
        "#verify split correctly\n",
        "print(f\"Train set: {len(train_df)} rows,\", f\"Validation set: {len(validation_df)} rows,\", f\"Test set: {len(test_df)}\")\n",
        "print(f\"Train: {len(train_df)/len(df)*100:.1f}%, \", f\"Validation: {len(validation_df)/len(df)*100:.1f}%, \", f\"Test: {len(test_df)/len(df)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "#2. Handle missing values (if any) and apply appropriate transformations:\n",
        "#• If categorical columns exist: apply one-hot encoding.\n",
        "\n",
        "df.isnull().sum()\n",
        "#So there is no missing values in this dataset\n",
        "\n",
        "categorical_columns = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "print(categorical_columns)\n",
        "#categorical_columns is empty, so there is no categorical columns in this dataset\n",
        "\n",
        "\n",
        "#• Create two interaction features of your choice; justify why they may help.\n",
        "\n",
        "# RM(average number of rooms per dwelling)and LSTAT(% lower status of the population) are two interaction features. \n",
        "# In a high-status neighborhood, where LSTAT is low, an extra room can add a lot of value. However, in a low-status area,\n",
        "# where LSTAT is high, an extra room may only add little value. So in the model, we want to show that the effect of room \n",
        "# numbers depends on the LSTAT. Adding the interaction RM × LSTAT to capture non-additive effects between house size and \n",
        "# neighborhood socioeconomic status can improve model expressiveness. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No Scaling:  (303, 14) (101, 14) (102, 14)\n",
            "Standard Scaling:  (303, 14) (101, 14) (102, 14)\n",
            "Robust Scaling:  (303, 14) (101, 14) (102, 14)\n"
          ]
        }
      ],
      "source": [
        "#3. Run the full pipeline under three scaling settings:\n",
        "\n",
        "X_train = train_df.to_numpy(dtype=float)\n",
        "X_validation = validation_df.to_numpy(dtype=float)\n",
        "X_test = test_df.to_numpy(dtype=float)\n",
        "\n",
        "#(a) No scaling\n",
        "train_no_scale, validation_no_scale, test_no_scale = X_train, X_validation, X_test\n",
        "\n",
        "#(b) Standard scaling (z-score)\n",
        "def standard_scaler_fit(X_train):\n",
        "    mu = X_train.mean(axis=0)\n",
        "    sigma = X_train.std(axis=0, ddof=0)\n",
        "    sigma[sigma == 0] = 1.0  # signma cannot be zero\n",
        "    return mu, sigma\n",
        "\n",
        "def standard_scaler_transform(X, mu, sigma):\n",
        "    return (X - mu) / sigma\n",
        "\n",
        "mu, sigma = standard_scaler_fit(X_train)\n",
        "\n",
        "train_standard_scaling = standard_scaler_transform(X_train, mu, sigma)\n",
        "validation_standard_scaling = standard_scaler_transform(X_validation, mu, sigma)\n",
        "test_standard_scaling = standard_scaler_transform(X_test, mu, sigma)\n",
        "\n",
        "#(c) Robust scaling (median/IQR)\n",
        "def robust_scaler_fit(X_train):\n",
        "    median = np.median(X_train, axis=0)\n",
        "    q1 = np.percentile(X_train, 25, axis=0)\n",
        "    q3 = np.percentile(X_train, 75, axis =0)\n",
        "    iqr =  q3 - q1\n",
        "    iqr[iqr == 0] = 1 # iqr cannot be zero\n",
        "    return median, iqr\n",
        "\n",
        "def robust_scaler_transform(X, median, iqr):\n",
        "    return (X-median) / iqr\n",
        "\n",
        "median, iqr = robust_scaler_fit(X_train)\n",
        "\n",
        "train_robust_scaling = robust_scaler_transform(X_train, median, iqr)\n",
        "validation_robust_scaling = robust_scaler_transform(X_validation, median, iqr)\n",
        "test_robust_scaling = robust_scaler_transform(X_test, median, iqr)\n",
        "\n",
        "print(\"No Scaling: \", train_no_scale.shape, validation_no_scale.shape, test_no_scale.shape)\n",
        "print(\"Standard Scaling: \", train_no_scale.shape, validation_no_scale.shape, test_no_scale.shape)\n",
        "print(\"Robust Scaling: \", train_no_scale.shape, validation_no_scale.shape, test_no_scale.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Condition number κ(X^T X):\n",
            "  Before scaling (no scaling):  7.2887e+07\n",
            "  After standard scaling:       1.2544e+02\n",
            "  After robust scaling:         2.3851e+03\n"
          ]
        }
      ],
      "source": [
        "#4. Compute and report:\n",
        "#• Condition number κ(X⊤X) (np.linalg.cond(XtX)) before and after scaling\n",
        "\n",
        "def condition_number(X):\n",
        "    \"\"\"Compute κ(X^T X) for design matrix X.\"\"\"\n",
        "    XtX = X.T @ X\n",
        "    return np.linalg.cond(XtX)\n",
        "\n",
        "kappa_no_scale = condition_number(train_no_scale)\n",
        "kappa_standard = condition_number(train_standard_scaling)\n",
        "kappa_robust = condition_number(train_robust_scaling)\n",
        "\n",
        "print(\"Condition number κ(X^T X):\")\n",
        "print(f\"  Before scaling (no scaling):  {kappa_no_scale:.4e}\")\n",
        "print(f\"  After standard scaling:       {kappa_standard:.4e}\")\n",
        "print(f\"  After robust scaling:         {kappa_robust:.4e}\")\n",
        "\n",
        "# Top-10 absolute correlations among features\n",
        "corr_matrix = np.corrcoef(train_no_scale.T)  # (n_features, n_features)\n",
        "n_features = corr_matrix.shape[0]\n",
        "# Collect (i, j, abs_corr) for i < j to avoid diagonal and duplicates\n",
        "pairs = []\n",
        "for i in range(n_features):\n",
        "    for j in range(i + 1, n_features):\n",
        "        pairs.append((i, j, abs(corr_matrix[i, j])))\n",
        "pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "top10 = pairs[:10]\n",
        "\n",
        "print(\"\\nTop-10 absolute correlations among features:\")\n",
        "print(f\"{'Feature 1':<12} {'Feature 2':<12} {'|correlation|':>14}\")\n",
        "print(\"-\" * 40)\n",
        "for i, j, ac in top10:\n",
        "    print(f\"{column_names[i]:<12} {column_names[j]:<12} {ac:>14.4f}\")\n",
        "\n",
        "# Correlation heatmap\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "im = ax.imshow(corr_matrix, cmap=\"RdBu_r\", vmin=-1, vmax=1, aspect=\"auto\")\n",
        "ax.set_xticks(range(n_features))\n",
        "ax.set_xticklabels(column_names, rotation=45, ha=\"right\")\n",
        "ax.set_yticks(range(n_features))\n",
        "ax.set_yticklabels(column_names)\n",
        "plt.colorbar(im, ax=ax, label=\"Correlation\")\n",
        "ax.set_title(\"Feature correlation matrix (training data)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
